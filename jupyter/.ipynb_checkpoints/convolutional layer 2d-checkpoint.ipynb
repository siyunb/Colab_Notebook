{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "optional-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "configured-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a fliter from cross correlation\n",
    "def corr2d(X, K):\n",
    "    l, w = K.shape[0], K.shape[1]\n",
    "    Y = torch.zeros(X.shape[0]-l+1, X.shape[1]-w+1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j] = (X[i:i+l, j:j+w]*K).sum()\n",
    "    return Y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "confident-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a fliter from convolutional view\n",
    "def corr2d_c(X, K):\n",
    "    l, w = K.shape[0], K.shape[1]\n",
    "    Y = torch.zeros(X.shape[0]-l+1, X.shape[1]-w+1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            summation = 0\n",
    "            for m in range(K.shape[0]):\n",
    "                for n in range(K.shape[1]):\n",
    "                    summation += X[i+m,j+n]*K[m,n]\n",
    "            Y[i,j] = summation\n",
    "    return Y    \n",
    "# I feel very well with this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "labeled-electricity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19., 25.],\n",
      "        [37., 43.]])\n",
      "tensor([[19., 25.],\n",
      "        [37., 43.]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[0,1,2],[3,4,5],[6,7,8]])\n",
    "K = torch.tensor([[0,1],[2,3]])\n",
    "print(corr2d(X, K))\n",
    "print(corr2d_c(X,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "seasonal-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size)) \n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "    def forward(self, X):\n",
    "        return self.corr2d(X, self.weight)+self.bias      \n",
    "    @staticmethod\n",
    "    def corr2d(X, K):\n",
    "        l, w = K.shape[0], K.shape[1]\n",
    "        Y = torch.zeros(X.shape[0]-l+1, X.shape[1]-w+1)\n",
    "        for i in range(Y.shape[0]):\n",
    "            for j in range(Y.shape[1]):\n",
    "                Y[i,j] = (X[i:i+l, j:j+w]*K).sum()\n",
    "        return Y  \n",
    "    \n",
    "#@classmethod隐含class的self被当作参数\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fixed-operator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(8,8, dtype = torch.float)\n",
    "X[:, 2:6] = 0.0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "static-prize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "K = torch.tensor([[1,-1]])\n",
    "Y = Conv2D.corr2d(X, K)\n",
    "print(Y)\n",
    "#it can represent the feature of space very well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "southeast-stuart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6230, 1.7045, 1.2588, 1.2588, 1.2588, 1.1774, 1.6230],\n",
       "        [1.6230, 1.7045, 1.2588, 1.2588, 1.2588, 1.1774, 1.6230],\n",
       "        [1.6230, 1.7045, 1.2588, 1.2588, 1.2588, 1.1774, 1.6230],\n",
       "        [1.6230, 1.7045, 1.2588, 1.2588, 1.2588, 1.1774, 1.6230],\n",
       "        [1.6230, 1.7045, 1.2588, 1.2588, 1.2588, 1.1774, 1.6230],\n",
       "        [1.6230, 1.7045, 1.2588, 1.2588, 1.2588, 1.1774, 1.6230],\n",
       "        [1.6230, 1.7045, 1.2588, 1.2588, 1.2588, 1.1774, 1.6230],\n",
       "        [1.6230, 1.7045, 1.2588, 1.2588, 1.2588, 1.1774, 1.6230]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = Conv2D((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "retained-satellite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4, loss:2.1933746337890625\n",
      "epoch:9, loss:0.13638180494308472\n",
      "epoch:14, loss:0.014653267338871956\n",
      "epoch:19, loss:0.002163947094231844\n",
      "epoch:24, loss:0.0003600973286665976\n",
      "epoch:29, loss:6.211607251316309e-05\n",
      "tensor([[ 1.0016, -1.0017]]) tensor([7.0605e-05])\n"
     ]
    }
   ],
   "source": [
    "# through the supervision learning to learn the parameter of the kernel\n",
    "# the standard kernel K\n",
    "K = torch.tensor([[1,-1]])\n",
    "Y = Conv2D.corr2d(X, K) # recall the staticmethod\n",
    "\n",
    "num_epoch = 30 \n",
    "lr = 0.01      \n",
    "conv2d = Conv2D((1,2))     \n",
    "\n",
    "#upgrade the kernel in conv2d\n",
    "for epoch in range(num_epoch):\n",
    "    Y_hat = conv2d(X)\n",
    "    loss = ((Y - Y_hat)**2).sum()\n",
    "    \n",
    "    if conv2d.weight.grad is not None:\n",
    "        for _, param in conv2d.named_parameters():\n",
    "            param.grad.zero_()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    for param in conv2d.parameters():\n",
    "        param.data -= lr*param.grad\n",
    "    \n",
    "    if (epoch+1)%5 == 0:\n",
    "        print('epoch:{0}, loss:{1}'.format(epoch, loss))\n",
    "print(conv2d.weight.data, conv2d.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "spiritual-greece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.4457, -0.0815]], requires_grad=True)),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([1.2588], requires_grad=True))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(conv2d.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "provincial-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding and Stride\n",
    "X = torch.rand(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "sudden-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 3, padding = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "veterinary-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_conv2d(conv2d_c, x_c):\n",
    "    x_c = x_c.view((1,1)+x_c.shape)\n",
    "    y = conv2d_c(x_c)\n",
    "    return y.view(y.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "graduate-silence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "muslim-strip",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size = (5,3), padding=(2,1))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "approximate-watershed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2330, -0.6291],\n",
       "        [-0.3928, -0.5024]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size = (3,5), padding=(0,1), stride=(3, 4))\n",
    "comp_conv2d(conv2d,X)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "conceptual-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a fliter from cross correlation\n",
    "def corr2d(X, K):\n",
    "    l, w = K.shape[0], K.shape[1]\n",
    "    Y = torch.zeros(X.shape[0]-l+1, X.shape[1]-w+1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j] = (X[i:i+l, j:j+w]*K).sum()\n",
    "    return Y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "settled-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X,K):\n",
    "    res = corr2d(X[0,:,:], K[0,:,:])\n",
    "    for i in range(1, X.shape[0]):\n",
    "        res += corr2d(X[i,:,:], K[i,:,:])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "baking-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[[0,1,2],[3,4,5],[6,7,8]],[[1,2,3],[4,5,6],[7,8,9]]])\n",
    "K = torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "promotional-switch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in(X,K).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "veterinary-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "independent-combine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "K = torch.stack([K,K+1,K+2])\n",
    "for k in K:\n",
    "    print(k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "restricted-conflict",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X, K).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "union-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1_1(X, K):\n",
    "    c,h,w = X.shape\n",
    "    X = X.view(c,-1)\n",
    "    K = K.view(K.shape[0],-1)\n",
    "    Y = torch.mm(K,X)\n",
    "    return Y.view(K.shape[0], h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "recovered-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(3,3,3)\n",
    "K = torch.rand(4,3,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "emotional-concern",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 3])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out_1_1(X,K).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "after-advocacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         [ True, False,  True]],\n",
       "\n",
       "        [[ True,  True,  True],\n",
       "         [ True,  True, False],\n",
       "         [False,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True],\n",
       "         [ True, False, False],\n",
       "         [ True,  True, False]],\n",
       "\n",
       "        [[ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X,K) == corr2d_multi_in_out_1_1(X,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-apartment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-third",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-baptist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

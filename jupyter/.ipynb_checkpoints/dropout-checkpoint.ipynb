{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader  \n",
    "from torch.nn import init\n",
    "import torch.optim as optim \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import time \n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, drop_prob):\n",
    "    X = X.float()\n",
    "    assert 0<=drop_prob<=1\n",
    "    keep_prob = 1- drop_prob\n",
    "    if keep_prob == 0:\n",
    "        return torch.zeros_like(X)\n",
    "    mask = (torch.rand(X.shape,dtype = torch.float)<keep_prob).float()\n",
    "    return mask*X/keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.arange(16).view(2,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "w1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens1)), dtype = torch.float,requires_grad = True)\n",
    "b1 = torch.zeros(num_hiddens1, requires_grad = True)\n",
    "w2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens1, num_hiddens2)), dtype = torch.float,requires_grad = True)\n",
    "b2 = torch.zeros(num_hiddens2, requires_grad = True)\n",
    "w3 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens2, num_outputs)), dtype = torch.float,requires_grad = True)\n",
    "b3 = torch.zeros(num_outputs, requires_grad = True)\n",
    "\n",
    "params = [w1, b1, w2, b2, w3, b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据切割\n",
    "def load_data_fashion_mnist(batch_size):\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root = './Dataset/FashionMNIST', train = True, \n",
    "                                                    download = True, transform = transforms.ToTensor())\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root = './Dataset/FashionMNIST', train = False, \n",
    "                                                   download = True, transform = transforms.ToTensor())\n",
    "    num_workers = 4\n",
    "    train_iter = DataLoader(mnist_train, batch_size = batch_size, shuffle = True, num_workers = num_workers)\n",
    "    test_iter = DataLoader(mnist_test, batch_size = batch_size, shuffle = True, num_workers = num_workers)\n",
    "    return train_iter, test_iter\n",
    "\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(X, params, drop_prob1, drop_prob2, is_training = True ):\n",
    "    X = X.view(-1, num_inputs)\n",
    "    w1, b1, w2, b2, w3, b3 = params\n",
    "    h1 = (torch.matmul(X, w1) + b1).relu()\n",
    "    if is_training:\n",
    "        h1 = dropout(h1, drop_prob1)\n",
    "    h2 = (torch.matmul(h1, w2) + b2).relu()\n",
    "    if is_training:\n",
    "        h2 = dropout(h2, drop_prob2)\n",
    "    return torch.matmul(h2, w3) + b3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    y_exp = y.exp()\n",
    "    partition = y_exp.sum(dim = 1, keepdim = True)\n",
    "    return (y_exp/partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr):\n",
    "    for param in params:\n",
    "        param.data -= lr*param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentrpyloss(y_hat, y):\n",
    "    return -(torch.log(y_hat.gather(1, y.view(-1,1))).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, params, drop_prob1, drop_prob2, network):\n",
    "    acc_num, n = 0, 0\n",
    "    for X, y in data_iter:\n",
    "        if isinstance(network, nn.Module):\n",
    "            network.eval()\n",
    "            acc_num += (network(X).argmax(dim = 1, keepdim = True) == y.view(-1,1)).float().sum().item()\n",
    "            network.train()\n",
    "        else:\n",
    "            if('is_training' in network.__code__.co_varnames):\n",
    "                acc_num += (network(X, params, drop_prob1, drop_prob2, is_training = False).argmax(dim = 1, keepdim = True) == y.view(-1,1)).float().sum().item()\n",
    "            else:\n",
    "                acc_num += (network(X).argmax(dim = 1, keepdim = True) == y.view(-1,1)).float().sum().item()\n",
    "        n += len(y)\n",
    "    return acc_num/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_dropout(n_epoch, drop_prob1, drop_prob2, \n",
    "                       train_iter, test_iter, \n",
    "                       model_fn, loss_fn, \n",
    "                       params = None, lr = None, optimizer = None):\n",
    "    for epoch in range(n_epoch):\n",
    "        train_loss, train_acc_num, n, i = 0.0, 0.0, 0, 0\n",
    "        for train_X, train_y in train_iter:\n",
    "            y_hat = softmax(model_fn(train_X, params, drop_prob1, drop_prob2))\n",
    "            loss = loss_fn(y_hat, train_y)\n",
    "            \n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if optimizer is not None:\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                sgd(params, lr)\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_acc_num += (y_hat.argmax(dim = 1) == train_y).float().sum().item()\n",
    "            n += len(train_y)\n",
    "            i+=1\n",
    "        if epoch%5 == 0:\n",
    "            test_loss, j = 0.0, 0\n",
    "            acc_test = evaluate_accuracy(test_iter, params, drop_prob1, drop_prob2, model_fn)\n",
    "            acc_train = train_acc_num/n\n",
    "            for test_X, test_y in test_iter:\n",
    "                j += 1\n",
    "                test_loss += loss_fn(softmax(model_fn(test_X, params, drop_prob1, drop_prob2, is_training = False)), test_y).item()\n",
    "            print('epoch: %d, acc_train: %.2f, acc_test: %.2f, train_loss: %.2f, test_loss, %.2f'% (epoch, acc_train, acc_test, train_loss/i, test_loss/j))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc_train: 0.89, acc_test: 0.87, train_loss: 0.31, test_loss, 0.36\n",
      "epoch: 5, acc_train: 0.89, acc_test: 0.87, train_loss: 0.29, test_loss, 0.37\n",
      "epoch: 10, acc_train: 0.90, acc_test: 0.88, train_loss: 0.28, test_loss, 0.33\n",
      "epoch: 15, acc_train: 0.90, acc_test: 0.88, train_loss: 0.26, test_loss, 0.33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0018, -0.0108,  0.0001,  ...,  0.0147, -0.0201, -0.0013],\n",
       "         [ 0.0044, -0.0094,  0.0123,  ...,  0.0019, -0.0214, -0.0026],\n",
       "         [-0.0109,  0.0055, -0.0287,  ..., -0.0040,  0.0047,  0.0048],\n",
       "         ...,\n",
       "         [-0.0208, -0.0056,  0.0045,  ...,  0.0038,  0.0127,  0.0063],\n",
       "         [-0.0134, -0.0008,  0.0065,  ...,  0.0019,  0.0055, -0.0002],\n",
       "         [-0.0061, -0.0039, -0.0010,  ...,  0.0008,  0.0105,  0.0035]],\n",
       "        requires_grad=True),\n",
       " tensor([-4.4109e-02,  2.1628e-02, -2.5281e-03,  5.1956e-03,  3.4606e-03,\n",
       "          1.7530e-01,  5.2560e-03, -8.0741e-02, -1.2479e-01,  1.0875e-03,\n",
       "          5.8505e-02, -5.6759e-02,  8.8973e-03,  9.6511e-02,  1.2680e-01,\n",
       "          2.0120e-01,  7.6371e-02,  1.3513e-01, -1.1088e-01,  2.7709e-02,\n",
       "          5.4412e-04,  3.8776e-02,  1.7626e-01,  1.6261e-02,  9.2947e-02,\n",
       "         -2.2806e-01,  7.3626e-02, -3.5554e-02,  1.6702e-01,  1.0270e-03,\n",
       "          5.4554e-02, -4.9989e-02,  1.4277e-01,  1.9133e-01,  1.5309e-03,\n",
       "          7.1574e-03,  2.9557e-02, -1.0198e-01,  1.6264e-02, -4.1605e-02,\n",
       "          1.2968e-02,  9.4577e-02, -1.2204e-01,  1.0959e-01, -2.9749e-04,\n",
       "         -4.5052e-02, -5.2774e-02,  5.0248e-02,  4.8763e-02,  3.0707e-02,\n",
       "          8.9575e-02, -1.3681e-02,  3.0316e-03,  2.0684e-01,  1.4470e-02,\n",
       "          8.1829e-04, -6.6997e-02, -1.0139e-01, -1.8023e-01, -7.8157e-03,\n",
       "          7.1460e-02,  5.9543e-03,  1.5918e-01, -7.4810e-02,  9.1898e-02,\n",
       "          6.9298e-02,  2.3905e-02,  2.2168e-02,  4.8538e-02,  1.0942e-01,\n",
       "         -5.8378e-02,  4.8395e-02,  5.2759e-02,  4.8548e-02, -1.3846e-01,\n",
       "         -6.3605e-02,  2.1239e-01,  6.1781e-02,  7.4977e-02, -8.3532e-02,\n",
       "          1.3624e-01,  1.6009e-01,  1.4229e-03,  2.1782e-01,  7.4795e-02,\n",
       "          6.4338e-02, -3.7364e-02, -2.0113e-02, -2.6247e-02,  2.3799e-01,\n",
       "          2.2418e-01,  2.2129e-01,  1.3035e-01, -1.9323e-01,  4.6362e-02,\n",
       "          1.1170e-01, -9.5545e-02,  1.3474e-01, -1.0458e-01,  2.4871e-01,\n",
       "          1.8861e-01,  1.0806e-02,  2.0253e-01,  5.9583e-02,  5.5166e-02,\n",
       "         -5.5753e-02, -3.4453e-03, -2.8641e-02, -4.2807e-02, -4.4159e-02,\n",
       "         -7.8851e-02,  8.6053e-02,  1.2498e-01,  6.3461e-02, -1.4884e-02,\n",
       "          1.0123e-01,  1.4965e-01, -5.6530e-02,  1.4316e-01, -3.6608e-02,\n",
       "         -7.6123e-03,  3.8177e-02, -3.0883e-04,  3.4251e-03,  1.1326e-02,\n",
       "          3.5300e-03,  2.0547e-01, -2.4420e-02, -3.0765e-03,  3.7447e-02,\n",
       "          7.3606e-02,  2.1542e-01, -4.8681e-02, -9.3919e-02,  3.0869e-03,\n",
       "          6.6328e-02, -3.4773e-02,  5.6185e-02, -8.7066e-02,  1.6583e-01,\n",
       "          4.0120e-02, -6.1335e-02, -4.3795e-02,  8.3623e-03, -5.6190e-02,\n",
       "          9.0703e-02, -2.9254e-02,  2.3545e-01,  2.2901e-01, -2.8793e-02,\n",
       "          1.5707e-04,  1.6726e-01,  2.9992e-01,  1.7561e-01,  8.4534e-02,\n",
       "          1.5420e-01, -3.9621e-02, -2.7590e-03, -5.0898e-02, -6.8447e-02,\n",
       "          3.5659e-02,  1.0609e-01,  3.5702e-02,  1.9402e-01,  9.4900e-02,\n",
       "          1.7043e-02, -4.7886e-02,  6.4782e-02,  1.0723e-01,  1.2429e-01,\n",
       "          1.9249e-01,  7.0154e-04, -8.5851e-02,  8.8667e-02, -4.1108e-04,\n",
       "         -8.5145e-02, -9.5867e-02,  1.3512e-04, -7.1539e-02,  6.3939e-02,\n",
       "         -1.1379e-01,  2.2768e-01,  1.2780e-01, -7.0134e-02, -8.8669e-02,\n",
       "          2.4657e-01,  1.4425e-01,  5.5656e-02,  6.8934e-02,  1.7520e-01,\n",
       "          2.1788e-01,  3.4592e-02, -8.9487e-02, -1.5846e-01,  1.3372e-02,\n",
       "         -1.0059e-02, -4.2644e-03,  1.5899e-01,  1.4504e-01,  2.2668e-03,\n",
       "         -9.7787e-02,  1.0862e-02, -2.7875e-02,  1.1640e-01,  1.0834e-01,\n",
       "          5.0145e-02, -8.3432e-02,  4.8968e-03,  3.3436e-02, -6.5998e-02,\n",
       "          1.9121e-01,  1.1891e-01,  1.3554e-01,  1.6996e-01,  1.3636e-01,\n",
       "          2.2261e-03, -4.8625e-02,  4.4341e-03, -1.8709e-01,  1.2814e-01,\n",
       "         -1.4741e-01,  7.3355e-02,  1.2668e-01, -1.0825e-01, -5.8834e-03,\n",
       "         -4.2808e-02, -1.0663e-01, -1.2945e-01,  2.3719e-02,  1.7820e-01,\n",
       "         -4.0935e-02,  1.8812e-01,  1.5938e-02, -5.9678e-02,  2.8927e-01,\n",
       "          1.3598e-01,  1.4298e-01, -1.8915e-01, -2.6697e-02, -3.8240e-02,\n",
       "         -5.2835e-02,  5.3595e-02,  5.2982e-02,  1.4269e-01, -6.3387e-04,\n",
       "          6.0383e-02, -7.8052e-02,  2.0362e-01, -5.8440e-02,  6.9333e-02,\n",
       "         -1.1544e-03,  2.0628e-01, -1.5977e-02,  1.0779e-01, -1.6496e-01,\n",
       "          4.4665e-02], requires_grad=True),\n",
       " tensor([[-0.0191, -0.0065, -0.0265,  ...,  0.0151,  0.0787, -0.0185],\n",
       "         [ 0.1034,  0.0512,  0.0813,  ..., -0.0006,  0.0385,  0.0445],\n",
       "         [-0.0259,  0.0046,  0.0048,  ..., -0.0126,  0.0174, -0.0015],\n",
       "         ...,\n",
       "         [ 0.0357, -0.0019,  0.0406,  ...,  0.0165,  0.0207,  0.0507],\n",
       "         [-0.0099, -0.0761,  0.0231,  ...,  0.0392, -0.0320,  0.0138],\n",
       "         [-0.0163, -0.0216,  0.0112,  ..., -0.0654, -0.0377,  0.0170]],\n",
       "        requires_grad=True),\n",
       " tensor([ 0.1247,  0.1252,  0.1065,  0.0078,  0.0202,  0.0917,  0.0890,  0.1438,\n",
       "          0.0143,  0.0525,  0.0209,  0.0711,  0.0603,  0.0753,  0.0706,  0.0076,\n",
       "          0.0809,  0.1220,  0.1495, -0.0005,  0.1552,  0.1186,  0.0245, -0.0099,\n",
       "          0.0995,  0.0842,  0.1062,  0.1576,  0.0784,  0.0233,  0.0632,  0.0298,\n",
       "          0.0743,  0.0848,  0.1067,  0.0262, -0.0181,  0.1242,  0.0148,  0.0623,\n",
       "         -0.0077,  0.0018, -0.0238,  0.0985,  0.0643,  0.1546,  0.0057,  0.0787,\n",
       "          0.0109,  0.0273,  0.2338,  0.1000,  0.0849,  0.0110,  0.0257,  0.0968,\n",
       "          0.1044,  0.0401,  0.0691,  0.1661, -0.0344,  0.0545,  0.0382,  0.0823,\n",
       "          0.0917, -0.0024, -0.0135,  0.0622,  0.1599,  0.0074,  0.0097,  0.1458,\n",
       "          0.0818,  0.0589,  0.1233,  0.1292,  0.0009,  0.1118,  0.1511,  0.0160,\n",
       "          0.0795,  0.1004,  0.1600,  0.0176,  0.1265,  0.0447,  0.0220,  0.0376,\n",
       "          0.1579,  0.0633,  0.0333,  0.0374, -0.0193,  0.0394,  0.0039,  0.0020,\n",
       "         -0.0115,  0.1671,  0.0805,  0.0706,  0.0098,  0.0874,  0.1042,  0.1204,\n",
       "          0.0343,  0.0632,  0.0936,  0.0761,  0.1192,  0.0355,  0.0327,  0.0432,\n",
       "          0.0492,  0.0076,  0.1258,  0.0087,  0.0619,  0.0785, -0.0193,  0.1076,\n",
       "          0.0654,  0.0206,  0.0918,  0.1065,  0.0178,  0.0411,  0.1199,  0.0229,\n",
       "          0.0029,  0.1068, -0.0046,  0.0683,  0.0082,  0.1081,  0.0461,  0.0729,\n",
       "         -0.0012,  0.0795,  0.0707,  0.0449,  0.2115,  0.0878,  0.0077,  0.0584,\n",
       "          0.1339,  0.0054,  0.0044,  0.0830,  0.1293,  0.0264,  0.0870,  0.0742,\n",
       "          0.1235,  0.1512,  0.0033,  0.0715,  0.0620,  0.1470,  0.0142,  0.1909,\n",
       "          0.0723,  0.0617,  0.1095,  0.1394,  0.1245,  0.1287,  0.1555,  0.0753,\n",
       "          0.0703,  0.0061,  0.0874,  0.0092, -0.0129,  0.0207,  0.1034,  0.0141,\n",
       "          0.0658,  0.0005,  0.1554,  0.1182,  0.0916,  0.0546,  0.1052,  0.1902,\n",
       "         -0.0105, -0.0006,  0.0428,  0.0595,  0.1095,  0.0693,  0.0415,  0.0933,\n",
       "          0.0856, -0.0446,  0.1618,  0.0411,  0.0074,  0.1207,  0.0562,  0.0212,\n",
       "          0.0263,  0.1675,  0.0936,  0.0943,  0.0061,  0.0185, -0.0177,  0.0020,\n",
       "          0.0757,  0.0313,  0.0379,  0.0921,  0.0409,  0.1091,  0.1100,  0.0730,\n",
       "          0.0611,  0.0763,  0.0359,  0.0494,  0.1256,  0.0549,  0.0415, -0.0014,\n",
       "          0.0566,  0.0709,  0.1322,  0.1005,  0.1350,  0.0887,  0.1583,  0.0377,\n",
       "         -0.0020,  0.1200,  0.0087,  0.1155,  0.0447, -0.0159,  0.2371,  0.1730,\n",
       "          0.0869,  0.1014,  0.1607,  0.1592, -0.0017,  0.0725,  0.1075,  0.1075,\n",
       "          0.1145,  0.0124,  0.0194,  0.0189,  0.0971,  0.0829,  0.1424,  0.1145],\n",
       "        requires_grad=True),\n",
       " tensor([[-0.1478, -0.4032, -0.2099,  ...,  0.1282,  0.0480,  0.1373],\n",
       "         [ 0.2791,  0.1899, -0.2680,  ..., -0.0810, -0.1909, -0.0350],\n",
       "         [-0.0901, -0.3354, -0.1952,  ...,  0.0997,  0.0200,  0.0584],\n",
       "         ...,\n",
       "         [-0.2472, -0.1424, -0.1458,  ...,  0.1254, -0.2932,  0.1570],\n",
       "         [ 0.0650,  0.0203, -0.2209,  ...,  0.3024,  0.2214,  0.3146],\n",
       "         [-0.2456, -0.3270, -0.2124,  ...,  0.1204, -0.0138,  0.1201]],\n",
       "        requires_grad=True),\n",
       " tensor([-0.1453, -0.1477,  0.2779,  0.1207, -0.8543,  1.5669,  0.1747,  0.3447,\n",
       "         -0.4336, -0.9039], requires_grad=True)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loop_dropout(n_epoch = 20, drop_prob1 = 0.2, drop_prob2 = 0.5, \n",
    "                       train_iter = train_iter, test_iter = test_iter, \n",
    "                       model_fn = network, loss_fn = crossentrpyloss, \n",
    "                       params = params, lr = 0.1, optimizer = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

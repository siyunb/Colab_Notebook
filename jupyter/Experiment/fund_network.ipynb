{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "enhanced-caution",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 载入python库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "distinct-disaster",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 载入python库\n",
    "import pandas as pd\n",
    "import xlrd, openpyxl\n",
    "import re\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-metallic",
   "metadata": {},
   "source": [
    "##  排除数据源中的干扰行函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-stewart",
   "metadata": {},
   "source": [
    "## 创建文件夹函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建所需的文件夹\n",
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "confident-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "#排除数据源中的干扰行\n",
    "def fliter_none(issue):\n",
    "    pattern = re.compile('\\d{6}.[A-Z]{2}')\n",
    "    try:\n",
    "        id_stock = pattern.search(issue)\n",
    "        if id_stock is not None :\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-canada",
   "metadata": {},
   "source": [
    "## 分别生成每只基金重仓股即其持股比例的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comfortable-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "#分别生成每只基金重仓股即其持股比例的列表\n",
    "def generate_dict(dataframe, point_filepath, csv_name):\n",
    "    \n",
    "    fund_network = {}\n",
    "    node_network = {'node': [], 'group': [], 'k_value': []}  #生成一个储存node信息的字典\n",
    "    for _, row in dataframe.iterrows():\n",
    "        fund, stock, group, fluent_rate, value_rate, k_value = row\n",
    "        if fund in fund_network:\n",
    "            fund_network[fund].append((stock, k_value))\n",
    "        else:\n",
    "            fund_network[fund] = []\n",
    "            fund_network[fund].append((stock, k_value))\n",
    "            node_network['node'].append(fund)\n",
    "            node_network['group'].append(group)\n",
    "    for fund in node_network['node']:\n",
    "        node_network['k_value'].append(fund_network[fund])\n",
    "    node_network = pd.DataFrame(node_network)\n",
    "    node_network.to_csv(point_filepath + '/' + csv_name + '_node_network' +'.csv',\n",
    "                        encoding='utf_8_sig',\n",
    "                        index=True)\n",
    "    return fund_network  #{基金：[(股票1:k1),(股票2：k2)，……]}基金持仓各个股票及k值列表，列表内元素为元组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exceptional-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "#统计两只基金相同持股的股票列表，若比例大于临界值则记为1，否则记为0\n",
    "def find_common(stock_list1, stock_list2, threshold):\n",
    "    coshare_element = []\n",
    "    for stock_name1, rate1 in stock_list1:  #[(股票1:k1)，(股票2：k2)，……]列表内每个元素由两部分组成\n",
    "        for stock_name2, rate2 in stock_list2:\n",
    "            if (stock_name1 == stock_name2) and ((rate1 > threshold) & (rate2>threshold)):\n",
    "                coshare_element.append((stock_name1, 1))\n",
    "            elif (stock_name1 == stock_name2) and ((rate1 <= threshold) | (rate2<=threshold)):\n",
    "                coshare_element.append((stock_name1, 0))\n",
    "    return coshare_element   #[(股票1:1)，(股票2:1)，(股票3:0)，……]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opponent-pricing",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#统计两只基金相同持股的股票数量及均大于临界值的股票数量\n",
    "def generate_network_dataframe(fund_network, fund_network_copy, threshold):\n",
    "    network_dataframe = {'source':[],'target':[],'coshare_stock':[],'coshare_num':[],'weight_stock':[],'weight':[]}\n",
    "    \n",
    "    for key1, stock_list1 in fund_network.items():\n",
    "        del fund_network_copy[key1]  #把自己删掉避免自己和自己比较\n",
    "        for key2, stock_list2 in fund_network_copy.items():\n",
    "            coshare_element = find_common(stock_list1, stock_list2, threshold) #[(股票1:1)，(股票2:1)，(股票3:0)，……]\n",
    "            \n",
    "            \n",
    "            if len(coshare_element)>0:\n",
    "                network_dataframe['source'].append(key1)\n",
    "                network_dataframe['target'].append(key2)\n",
    "                network_dataframe['coshare_stock'].append('*'.join(sorted([share_stock[0] for share_stock in coshare_element])))\n",
    "                network_dataframe['coshare_num'].append(len(coshare_element))\n",
    "                network_dataframe['weight_stock'].append('*'.join(sorted([share_stock[0] for share_stock in coshare_element if share_stock[1]==1])))\n",
    "                network_dataframe['weight'].append(sum([share_stock[1] for share_stock in coshare_element]))\n",
    "                \n",
    "                \n",
    "    network_dataframe = pd.DataFrame(network_dataframe)                            #return 这个是全weight数据\n",
    "    network_dataframe_weight = network_dataframe[network_dataframe['weight']!=0]   #return 这个是删除weight为0的\n",
    "    \n",
    "    output_list = ['source','target','weight_stock','weight']\n",
    "    return network_dataframe_weight[output_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "coordinated-hundred",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#共同持有重仓股排名列表\n",
    "def generate_weighted_stock(fund_network, fund_network_copy, threshold):\n",
    "    #判断是否存在路径\n",
    "\n",
    "    weighted_stock = {}  ##形成重仓股排名列表  {股票1：3，股票2：6,……}\n",
    "    for key1, stock_list1 in fund_network.items():\n",
    "        del fund_network_copy[key1]  #把自己删掉避免自己和自己比较\n",
    "        for key2, stock_list2 in fund_network_copy.items():\n",
    "            coshare_element = find_common(stock_list1, stock_list2, threshold) #[(股票1:1)，(股票2:1)，(股票3:0)，……]\n",
    "            \n",
    "            for share_stock in coshare_element:\n",
    "                if share_stock[1]==1:\n",
    "                    if share_stock[0] in weighted_stock.keys():\n",
    "                        weighted_stock[share_stock[0]] +=1\n",
    "                    else:\n",
    "                        weighted_stock[share_stock[0]] = 1\n",
    "    global weighted_stock_order  \n",
    "    weighted_stock_order = sorted(weighted_stock.items(),key=lambda x:x[1],reverse=False)                  \n",
    "    \n",
    "    return weighted_stock_order           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "solid-individual",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#单独持有重仓股排名列表\n",
    "def generate_important_stock(fund_network,threshold):\n",
    "    global important_stock     \n",
    "    important_stock = {} #形成重仓股排名列表  {股票1：3，股票2：6,……} \n",
    "    \n",
    "    for key1, stock_list1 in fund_network.items():  #{基金：[(股票1:k1),(股票2：k2)，……]}\n",
    "        for stock_name1, rate1 in stock_list1:      #[(股票1:k1)，(股票2：k2)，……]列表内每个元素由两部分组成\n",
    "            if rate1 > threshold:\n",
    "                if stock_name1 in important_stock.keys():\n",
    "                    important_stock[stock_name1] += 1\n",
    "                else:\n",
    "                    important_stock[stock_name1] = 1\n",
    "      \n",
    "    important_stock_order = sorted(important_stock.items(),key=lambda x:x[1],reverse=False)                  \n",
    "    \n",
    "    return important_stock_order\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "tired-workplace",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#生成网络结构数据\n",
    "def xlsx_csv_fund(xlsx_filepath, csv_filepath, fund_filepath, point_filepath, total_filepath,\n",
    "                  weighted_stock_filepath, important_stock_filepath, hyper_params, invest_category = None):\n",
    "    #创建所需文件夹\n",
    "    \n",
    "    \n",
    "    #获取文件路径\n",
    "    list_dir =  [os.path.splitext(dir) for dir in os.listdir(xlsx_filepath)]\n",
    "    \n",
    "    #确认成的网络\n",
    "    if invest_category is not None:\n",
    "        print('正在生成'+'+'.join(invest_category)+'投资的网络')\n",
    "    else:\n",
    "        print('正在生成全部投资的网络')\n",
    "    \n",
    "    total_weighted_stock = {}\n",
    "    total_important_stock ={}\n",
    "    for tuple_dir in list_dir:   #逐次针对当前日期\n",
    "        #读取wind数据\n",
    "        date_name, format_name = tuple_dir\n",
    "        dataframe = pd.read_excel(xlsx_filepath+'/'+date_name+format_name)\n",
    "        \n",
    "        #删去不符合投资类型的股票，并检验是否数据框为空，为空则跳转到下一个循环\n",
    "        if invest_category is not None:\n",
    "            dataframe = dataframe[dataframe['投资类型'].isin(invest_category)]\n",
    "        if dataframe.empty:\n",
    "            continue\n",
    "        \n",
    "        #从原始数据提取必要信息\n",
    "        column_name = ['代码','名称','股票代码','股票简称','持股占流通股比(%)','持股市值占基金净值比(%)','管理公司']\n",
    "        data_csv = dataframe[dataframe['代码'].apply(fliter_none)][column_name]\n",
    "        data_csv = data_csv.rename(columns = {'代码':'code',\n",
    "                                              '名称':'source',\n",
    "                                              '股票代码':'stock_code',\n",
    "                                              '股票简称':'target', \n",
    "                                              '持股占流通股比(%)':'fluent_rate',\n",
    "                                              '持股市值占基金净值比(%)':'value_rate', \n",
    "                                              '管理公司':'group'})\n",
    "        csv_name = ('_').join(date_name.split('-'))\n",
    "        dataframe_fund = data_csv[['source','target','group','fluent_rate','value_rate']]\n",
    "        \n",
    "        #生成k值进一步完善dataframe_fund\n",
    "        a, b, c = hyper_params\n",
    "        dataframe_fund_copy = dataframe_fund.copy()\n",
    "        #生成k值\n",
    "        dataframe_fund_copy['k_value'] = dataframe_fund.apply(lambda row: round(a*row[3]+b*row[4]+c*row[3]*row[4], 2), axis=1)\n",
    "        dataframe_fund_copy.to_csv(csv_filepath+'/'+csv_name+'.csv', encoding='utf_8_sig', index=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        fund_network = generate_dict(dataframe_fund_copy, point_filepath, csv_name)\n",
    "        fund_network_copy = deepcopy(fund_network)\n",
    "        threshold = hyper_threshold\n",
    "        \n",
    "        #生成制作网络的数据表\n",
    "        network_dataframe = generate_network_dataframe(fund_network,fund_network_copy,threshold)\n",
    "        network_dataframe.to_csv(fund_filepath+'/'+csv_name+'.csv', encoding='utf_8_sig', index=True)\n",
    "\n",
    "        #生成共同重仓股列表\n",
    "        fund_network_copy = deepcopy(fund_network)\n",
    "        global weighted_stock_dataframe\n",
    "        weighted_stock_dataframe = generate_weighted_stock(fund_network,fund_network_copy,threshold)\n",
    "        with open(weighted_stock_filepath+'/'+csv_name+'.csv', 'w', encoding = 'utf_8_sig',newline='') as f:     \n",
    "            csv_write = csv.writer(f)\n",
    "            for new_line in weighted_stock_dataframe:\n",
    "                csv_write.writerow(new_line)                        \n",
    "                               ####用此方法可以将字典里的元素逐行读入；否则生成一个横向很长的列表。（转置作用）\n",
    "        \n",
    "        #将共同重仓股信息汇总在一张表\n",
    "        total_weighted_stock[csv_name]=[]  ###注意必须先定义字典的值为一个列表，才能向列表里加元素!!!\n",
    "        for tuple_stock in weighted_stock_dataframe:\n",
    "            total_weighted_stock[csv_name].append(tuple_stock[0])\n",
    "        \n",
    "        \n",
    "        #生成单独重仓股列表\n",
    "        global important_stock_dataframe\n",
    "        important_stock_dataframe = generate_important_stock(fund_network,threshold)\n",
    "        with open(important_stock_filepath+'/'+csv_name+'.csv', 'w', encoding = 'utf_8_sig',newline='') as f:     \n",
    "            csv_write = csv.writer(f)\n",
    "            for new_line in important_stock_dataframe:\n",
    "                csv_write.writerow(new_line)       \n",
    "        \n",
    "        #将单独重仓股信息汇总在一张表\n",
    "        total_important_stock[csv_name]=[]  ###注意必须先定义字典的值为一个列表，才能向列表里加元素!!!\n",
    "        for tuple_stock in important_stock_dataframe:\n",
    "            total_important_stock[csv_name].append(tuple_stock[0])\n",
    "       \n",
    "            \n",
    "    \n",
    "    total_weighted_stock = pd.DataFrame(total_weighted_stock)\n",
    "    total_important_stock = pd.DataFrame(total_important_stock)\n",
    "    \n",
    "    total_weighted_stock.to_csv(total_filepath+'/'+'total_weighted_stock.csv', encoding='utf_8_sig', index=True) \n",
    "    total_important_stock.to_csv(total_filepath+'/'+'total_important_stock.csv', encoding='utf_8_sig', index=True)\n",
    "            \n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "public-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数设置\n",
    "xlsx_filepath = './Dataset/StockData'\n",
    "#csv_filepath = './Dataset/StockData_csv'\n",
    "fund_filepath = './Dataset/StockData_fund'\n",
    "point_filepath = './Dataset/StockData_point'\n",
    "weighted_stock_filepath = './Dataset/StockData_weighted_stock'\n",
    "important_stock_filepath = './Dataset/StockData_important_stock'\n",
    "total_filepath = './Dataset/StockData_total_stock'\n",
    "\n",
    "hyper_params = [0.2, 0.8, 0.5]\n",
    "\n",
    "hyper_threshold = 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decimal-aging",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xlsx_csv_fund' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f64d0ce8af69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mVersion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"last\"\u001b[0m \u001b[1;31m#@param [\"SAEHD\",\"Quick96\"]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m xlsx_csv_fund(xlsx_filepath, csv_filepath, fund_filepath, point_filepath, total_filepath,\n\u001b[0m\u001b[0;32m      3\u001b[0m               \u001b[0mweighted_stock_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimportant_stock_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m               invest_category=['普通股票型基金','指数增强型基金','灵活配置型基金','偏股混合型基金','平衡混合型基金','偏债混合型基金'])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xlsx_csv_fund' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "xlsx_csv_fund(xlsx_filepath, csv_filepath, fund_filepath, point_filepath, total_filepath,\n",
    "              weighted_stock_filepath, important_stock_filepath, hyper_params, \n",
    "              invest_category=['普通股票型基金','指数增强型基金','灵活配置型基金','偏股混合型基金','平衡混合型基金','偏债混合型基金'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "silent-sterling",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2003_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>招商银行</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  2003_1\n",
       "0   招商银行"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    url_pool = list_of_groups(complete_url,1)\n",
    "    p=Pool(1)\n",
    "    p.map(pool_url_bu, url_pool1)\n",
    "    p.close()\n",
    "    p.join() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-particle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-feeding",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

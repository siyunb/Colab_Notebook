{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc579a3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 载入python库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a9879a94",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 载入python库\n",
    "import pandas as pd\n",
    "import xlrd, openpyxl\n",
    "import re\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import csv\n",
    "import pickle as pkl\n",
    "import json\n",
    "from tqdm.notebook import tqdm, tnrange\n",
    "from multiprocessing.dummy import Pool "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc448f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##  排除数据源中的干扰行函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da463f48",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#排除数据源中的干扰行\n",
    "def fliter_none(issue):\n",
    "    pattern = re.compile('\\d{6}.[A-Z]{2}')\n",
    "    try:\n",
    "        id_stock = pattern.search(issue)\n",
    "        if id_stock is not None :\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f736f6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 创建文件夹函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c376f43b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#创建所需的文件夹\n",
    "def mkdir(xlsx_filepath):\n",
    "    root_path = '/'.join(xlsx_filepath.split('/')[:-1])\n",
    "    \n",
    "    csv_filepath = root_path + '/StockData_csv'\n",
    "    fund_filepath = root_path + '/StockData_fund'\n",
    "    point_filepath =  root_path + '/StockData_point'\n",
    "    weighted_stock_filepath = root_path + '/StockData_weighted_stock'\n",
    "    important_stock_filepath = root_path + '/StockData_important_stock'\n",
    "    total_filepath = root_path + '/StockData_total_stock'\n",
    "    \n",
    "    filepath_list = [xlsx_filepath,csv_filepath,fund_filepath,point_filepath,\n",
    "                     weighted_stock_filepath,important_stock_filepath,total_filepath]\n",
    "    \n",
    "    for filepath in filepath_list:\n",
    "        if not os.path.exists(filepath):\n",
    "            os.makedirs(filepath)\n",
    "        print('已建立', filepath)\n",
    "    \n",
    "    return filepath_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194378c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 分别生成每只基金重仓股即其持股比例的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "723f85c8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#分别生成每只基金重仓股即其持股比例的列表\n",
    "def generate_dict(dataframe, node_network_csv_path, fund_network_pkl_path):\n",
    "    \n",
    "    fund_network = {}\n",
    "    node_network = {'node': [], 'group': [], 'k_value': []}  #生成一个储存node信息的字典\n",
    "    for _, row in dataframe.iterrows():\n",
    "        fund, stock, group, fluent_rate, value_rate, k_value = row\n",
    "        if fund in fund_network:\n",
    "            fund_network[fund].append((stock, k_value))\n",
    "        else:\n",
    "            fund_network[fund] = []\n",
    "            fund_network[fund].append((stock, k_value))\n",
    "            node_network['node'].append(fund)\n",
    "            node_network['group'].append(group)\n",
    "    for fund in node_network['node']:\n",
    "        node_network['k_value'].append(fund_network[fund])\n",
    "    node_network = pd.DataFrame(node_network)\n",
    "    node_network.to_csv(node_network_csv_path, encoding='utf_8_sig',index=True)\n",
    "    \n",
    "    with open(fund_network_pkl_path, 'wb') as f:   #with：上下文管理器\n",
    "        pkl.dump(fund_network, f)\n",
    "    \n",
    "    return fund_network  #{基金：[(股票1:k1),(股票2：k2)，……]}基金持仓各个股票及k值列表，列表内元素为元组"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf05c0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 统计两只基金相同持股的股票列表，若比例大于临界值则记为1，否则记为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f23f6634",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#统计两只基金相同持股的股票列表，若比例大于临界值则记为1，否则记为0\n",
    "def find_common(stock_list1, stock_list2, threshold):\n",
    "    coshare_element = []\n",
    "    for stock_name1, rate1 in stock_list1:  #[(股票1:k1)，(股票2：k2)，……]列表内每个元素由两部分组成\n",
    "        for stock_name2, rate2 in stock_list2:\n",
    "            if (stock_name1 == stock_name2) and ((rate1 > threshold) & (rate2>threshold)):\n",
    "                coshare_element.append((stock_name1, 1))\n",
    "            elif (stock_name1 == stock_name2) and ((rate1 <= threshold) | (rate2<=threshold)):\n",
    "                coshare_element.append((stock_name1, 0))\n",
    "    return coshare_element   #[(股票1:1)，(股票2:1)，(股票3:0)，……]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ef196",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 统计两只基金相同持股的股票数量及均大于临界值的股票数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d151cbf8",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#统计两只基金相同持股的股票数量及均大于临界值的股票数量\n",
    "def generate_network_dataframe(fund_network, fund_network_copy, threshold):\n",
    "    network_dataframe = {'source':[],'target':[],'coshare_stock':[],'coshare_num':[],'weight_stock':[],'weight':[]}\n",
    "    \n",
    "    for key1, stock_list1 in fund_network.items():\n",
    "        del fund_network_copy[key1]  #把自己删掉避免自己和自己比较\n",
    "        for key2, stock_list2 in fund_network_copy.items():\n",
    "            coshare_element = find_common(stock_list1, stock_list2, threshold) #[(股票1:1)，(股票2:1)，(股票3:0)，……]\n",
    "            \n",
    "            \n",
    "            if len(coshare_element)>0:\n",
    "                network_dataframe['source'].append(key1)\n",
    "                network_dataframe['target'].append(key2)\n",
    "                network_dataframe['coshare_stock'].append('*'.join(sorted([share_stock[0] for share_stock in coshare_element])))\n",
    "                network_dataframe['coshare_num'].append(len(coshare_element))\n",
    "                network_dataframe['weight_stock'].append('*'.join(sorted([share_stock[0] for share_stock in coshare_element if share_stock[1]==1])))\n",
    "                network_dataframe['weight'].append(sum([share_stock[1] for share_stock in coshare_element]))\n",
    "                \n",
    "                \n",
    "    network_dataframe = pd.DataFrame(network_dataframe)                            #return 这个是全weight数据\n",
    "    network_dataframe_weight = network_dataframe[network_dataframe['weight']!=0]   #return 这个是删除weight为0的\n",
    "    \n",
    "    output_list = ['source','target','weight_stock','weight']\n",
    "    return network_dataframe_weight[output_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c15904",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 共同持有重仓股排名列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "355cf225",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#共同持有重仓股排名列表\n",
    "def generate_weighted_stock(fund_network, fund_network_copy, threshold):\n",
    "    weighted_stock = {}  ##形成重仓股排名列表  {股票1：3，股票2：6,……}\n",
    "    for key1, stock_list1 in fund_network.items():\n",
    "        del fund_network_copy[key1]  #把自己删掉避免自己和自己比较\n",
    "        for key2, stock_list2 in fund_network_copy.items():\n",
    "            coshare_element = find_common(stock_list1, stock_list2, threshold) #[(股票1:1)，(股票2:1)，(股票3:0)，……]\n",
    "            \n",
    "            for share_stock in coshare_element:\n",
    "                if share_stock[1]==1:\n",
    "                    if share_stock[0] in weighted_stock.keys():\n",
    "                        weighted_stock[share_stock[0]] +=1\n",
    "                    else:\n",
    "                        weighted_stock[share_stock[0]] = 1\n",
    "    weighted_stock_order = sorted(weighted_stock.items(),key=lambda x:x[1],reverse=False)                  \n",
    "    \n",
    "    return weighted_stock_order           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb98cb9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 单独持有重仓股排名列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7656c418",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#单独持有重仓股排名列表\n",
    "def generate_important_stock(fund_network,threshold):\n",
    "    important_stock = {} #形成重仓股排名列表  {股票1：3，股票2：6,……} \n",
    "    \n",
    "    for key1, stock_list1 in fund_network.items():  #{基金：[(股票1:k1),(股票2：k2)，……]}\n",
    "        for stock_name1, rate1 in stock_list1:      #[(股票1:k1)，(股票2：k2)，……]列表内每个元素由两部分组成\n",
    "            if rate1 > threshold:\n",
    "                if stock_name1 in important_stock.keys():\n",
    "                    important_stock[stock_name1] += 1\n",
    "                else:\n",
    "                    important_stock[stock_name1] = 1\n",
    "      \n",
    "    important_stock_order = sorted(important_stock.items(),key=lambda x:x[1],reverse=False)                  \n",
    "    \n",
    "    return important_stock_order\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-philip",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 生成并行的分块数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "behind-lighting",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sub_list(init_list, number_thread):\n",
    "    end_list = [list(i) for i in zip(*zip(*[iter(init_list)] *number_thread))]\n",
    "    count = len(init_list)%number_thread\n",
    "    if count!=0:   \n",
    "        for k,v in enumerate(init_list[-count:]):\n",
    "            end_list[k].append(v) \n",
    "    return end_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bead1d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 生成网络结构数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3faf4004",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#生成网络结构数据\n",
    "def xlsx_csv_fund(xlsx_filepath, hyper_params, hyper_threshold, invest_category = None):\n",
    "    \n",
    "    #创建所需文件夹\n",
    "    xlsx_filepath,csv_filepath,fund_filepath,point_filepath, \\\n",
    "    weighted_stock_filepath,important_stock_filepath,total_filepath = mkdir(xlsx_filepath)\n",
    "    \n",
    "    #获取文件路径\n",
    "    list_dir =  [os.path.splitext(dir_) for dir_ in os.listdir(xlsx_filepath)]\n",
    "    \n",
    "    #确认成的网络并且生成label\n",
    "    if invest_category is not None:\n",
    "        print('正在生成'+'+'.join(invest_category)+'投资的网络')\n",
    "    else:\n",
    "        print('正在生成全部投资的网络')\n",
    "        \n",
    "    total_weighted_stock = {}\n",
    "    total_important_stock ={}\n",
    "    for tuple_dir in tqdm(list_dir):   #逐次针对当前日期\n",
    "        #读取wind数据\n",
    "        date_name, format_name = tuple_dir\n",
    "        dataframe = pd.read_excel(xlsx_filepath+'/'+date_name+format_name)\n",
    "        \n",
    "        #删去不符合投资类型的股票，并检验是否数据框为空，为空则跳转到下一个循环\n",
    "        if invest_category is not None:\n",
    "            dataframe = dataframe[dataframe['投资类型'].isin(invest_category)]\n",
    "        if dataframe.empty:\n",
    "            continue\n",
    "        \n",
    "        #如果存在csv文件则直接读取，如果不存在则生成csv\n",
    "        csv_name = ('_').join(date_name.split('-'))\n",
    "        data_csv_path = csv_filepath+'/'+csv_name+'.csv'\n",
    "        if not os.path.exists(data_csv_path):\n",
    "            #从原始数据提取必要信息\n",
    "            column_name = ['代码','名称','股票代码','股票简称','持股占流通股比(%)','持股市值占基金净值比(%)','管理公司']\n",
    "            data_csv = dataframe[dataframe['代码'].apply(fliter_none)][column_name]\n",
    "            data_csv = data_csv.rename(columns = {'代码':'code',\n",
    "                                                  '名称':'source',\n",
    "                                                  '股票代码':'stock_code',\n",
    "                                                  '股票简称':'target', \n",
    "                                                  '持股占流通股比(%)':'fluent_rate',\n",
    "                                                  '持股市值占基金净值比(%)':'value_rate', \n",
    "                                                  '管理公司':'group'})\n",
    "            dataframe_fund = data_csv[['source','target','group','fluent_rate','value_rate']]\n",
    "            \n",
    "            #生成k值进一步完善dataframe_fund\n",
    "            a, b, c = hyper_params\n",
    "            dataframe_fund_copy = dataframe_fund.copy()\n",
    "            #生成k值\n",
    "            dataframe_fund_copy['k_value'] = dataframe_fund.apply(lambda row: round(a*row[3]+b*row[4]+c*row[3]*row[4], 2), axis=1)\n",
    "            dataframe_fund_copy.to_csv(data_csv_path, encoding='utf_8_sig', index=True)\n",
    "        else:\n",
    "            print('已存在',csv_name)\n",
    "            dataframe_fund_copy = pd.read_csv(data_csv_path, index_col = 0)\n",
    "        \n",
    "        #生成基金字典并储存基金网络\n",
    "        pkl_filepath = point_filepath + '/Pickle'\n",
    "        node_network_csv_path = point_filepath + '/' + csv_name + '_node_network' +'.csv'\n",
    "        fund_network_pkl_path = pkl_filepath +  '/' + csv_name + '_fund_network' +'.pkl'\n",
    "        if not os.path.exists(pkl_filepath):\n",
    "            os.makedirs(pkl_filepath)\n",
    "        if os.path.exists(node_network_csv_path):\n",
    "            with open(fund_network_pkl_path,'rb') as f:   \n",
    "                fund_network = pkl.load(f)   \n",
    "        else:   \n",
    "            fund_network = generate_dict(dataframe_fund_copy, node_network_csv_path, fund_network_pkl_path)\n",
    "        fund_network_copy = deepcopy(fund_network)\n",
    "        threshold = hyper_threshold\n",
    "        \n",
    "        #生成制作网络的数据表\n",
    "        network_path = fund_filepath+'/'+csv_name+'.csv'\n",
    "        if not os.path.exists(network_path):\n",
    "            network_dataframe = generate_network_dataframe(fund_network,fund_network_copy,threshold)\n",
    "            network_dataframe.to_csv(network_path, encoding='utf_8_sig', index=True)\n",
    "        else:\n",
    "            network_dataframe = pd.read_csv(network_path, index_col = 0)\n",
    "\n",
    "        #生成共同重仓股列表\n",
    "        json_weighted_filepath = weighted_stock_filepath + '/Json'\n",
    "        weighted_order_csv_path = weighted_stock_filepath+'/'+csv_name+'.csv'\n",
    "        weighted_order_json_path = json_weighted_filepath+'/'+csv_name+'.json'   \n",
    "        fund_network_copy = deepcopy(fund_network)\n",
    "        if not os.path.exists(json_weighted_filepath):\n",
    "            os.makedirs(json_weighted_filepath)\n",
    "        if not os.path.exists(weighted_order_csv_path):\n",
    "            weighted_stock_dataframe = generate_weighted_stock(fund_network,fund_network_copy,threshold)\n",
    "            with open(weighted_order_csv_path, 'w', encoding = 'utf_8_sig',newline='') as f:     \n",
    "                csv_write = csv.writer(f)\n",
    "                for new_line in weighted_stock_dataframe:\n",
    "                    csv_write.writerow(new_line) \n",
    "            with open(weighted_order_json_path,'w', encoding = 'utf_8_sig') as f:   \n",
    "                f.write(json.dumps(weighted_stock_dataframe))\n",
    "        else:\n",
    "            with open(weighted_order_json_path,'r', encoding = 'utf_8_sig') as f:   \n",
    "                weighted_stock_dataframe = f.read()\n",
    "                               ####用此方法可以将字典里的元素逐行读入；否则生成一个横向很长的列表。（转置作用）\n",
    "\n",
    "        #生成单独重仓股列表\n",
    "        json_important_filepath = important_stock_filepath + '/Json'\n",
    "        important_stock_csv_path = important_stock_filepath+'/'+csv_name+'.csv'\n",
    "        important_order_json_path = json_important_filepath+'/'+csv_name+'.json' \n",
    "        if not os.path.exists(json_important_filepath):\n",
    "            os.makedirs(json_important_filepath)\n",
    "        if not os.path.exists(important_stock_csv_path):\n",
    "            important_stock_dataframe = generate_important_stock(fund_network,threshold)\n",
    "            with open(important_stock_csv_path, 'w', encoding = 'utf_8_sig',newline='') as f:     \n",
    "                csv_write = csv.writer(f)\n",
    "                for new_line in important_stock_dataframe:\n",
    "                    csv_write.writerow(new_line)   \n",
    "            with open(important_order_json_path,'w', encoding = 'utf_8_sig') as f:   \n",
    "                f.write(json.dumps(important_stock_dataframe))  \n",
    "        else:\n",
    "            with open(important_order_json_path,'r', encoding = 'utf_8_sig') as f:   \n",
    "                important_stock_dataframe = f.read()\n",
    "\n",
    "        #将共同重仓股信息汇总在一张表\n",
    "        total_weighted_stock[csv_name]=[]  ###注意必须先定义字典的值为一个列表，才能向列表里加元素!!!\n",
    "        for tuple_stock in weighted_stock_dataframe:\n",
    "            total_weighted_stock[csv_name].append(tuple_stock[0])\n",
    "        \n",
    "        #将单独重仓股信息汇总在一张表\n",
    "        total_important_stock[csv_name]=[]  ###注意必须先定义字典的值为一个列表，才能向列表里加元素!!!\n",
    "        for tuple_stock in important_stock_dataframe:\n",
    "            total_important_stock[csv_name].append(tuple_stock[0])\n",
    "           \n",
    "    total_weighted_stock = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in total_weighted_stock.items()]))\n",
    "    total_important_stock = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in total_important_stock.items()]))\n",
    "    \n",
    "    total_weighted_stock.to_csv(total_filepath+'/'+'total_weighted_stock.csv', encoding='utf_8_sig', index=True) \n",
    "    total_important_stock.to_csv(total_filepath+'/'+'total_important_stock.csv', encoding='utf_8_sig', index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-password",
   "metadata": {},
   "source": [
    "## 并行生成网络结构数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "closed-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成网络结构数据\n",
    "def xlsx_csv_fund_parallel(xlsx_filepath, hyper_params, hyper_threshold, worker = 4, invest_category = None):\n",
    "    \n",
    "    #创建所需文件夹\n",
    "    global csv_filepath,fund_filepath,point_filepath,weighted_stock_filepath,important_stock_filepath,total_filepath\n",
    "    xlsx_filepath,csv_filepath,fund_filepath,point_filepath, \\\n",
    "    weighted_stock_filepath,important_stock_filepath,total_filepath = mkdir(xlsx_filepath)\n",
    "    \n",
    "    #获取文件路径\n",
    "    list_dir =  [os.path.splitext(dir_) for dir_ in os.listdir(xlsx_filepath)]\n",
    "    \n",
    "    #确认成的网络\n",
    "    if invest_category is not None:\n",
    "        print('正在生成'+'+'.join(invest_category)+'投资的网络')\n",
    "    else:\n",
    "        print('正在生成全部投资的网络')\n",
    "    \n",
    "    #生成每个线程的任务\n",
    "    mission_list = sub_list(list_dir, worker)\n",
    "    mul_thread = Pool(worker)\n",
    "    mul_thread.map(mission_fn, mission_list)\n",
    "    mul_thread.close()\n",
    "    mul_thread.join() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "baking-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mission_fn(list_dir):\n",
    "    total_weighted_stock = {}\n",
    "    total_important_stock ={}\n",
    "    for tuple_dir in tqdm(list_dir):   #逐次针对当前日期\n",
    "        #读取wind数据\n",
    "        date_name, format_name = tuple_dir\n",
    "        dataframe = pd.read_excel(xlsx_filepath+'/'+date_name+format_name)\n",
    "        \n",
    "        #删去不符合投资类型的股票，并检验是否数据框为空，为空则跳转到下一个循环\n",
    "        if invest_category is not None:\n",
    "            dataframe = dataframe[dataframe['投资类型'].isin(invest_category)]\n",
    "        if dataframe.empty:\n",
    "            continue\n",
    "        \n",
    "        #如果存在csv文件则直接读取，如果不存在则生成csv\n",
    "        csv_name = ('_').join(date_name.split('-'))\n",
    "        data_csv_path = csv_filepath+'/'+csv_name+'.csv'\n",
    "        if not os.path.exists(data_csv_path):\n",
    "            #从原始数据提取必要信息\n",
    "            column_name = ['代码','名称','股票代码','股票简称','持股占流通股比(%)','持股市值占基金净值比(%)','管理公司']\n",
    "            data_csv = dataframe[dataframe['代码'].apply(fliter_none)][column_name]\n",
    "            data_csv = data_csv.rename(columns = {'代码':'code',\n",
    "                                                  '名称':'source',\n",
    "                                                  '股票代码':'stock_code',\n",
    "                                                  '股票简称':'target', \n",
    "                                                  '持股占流通股比(%)':'fluent_rate',\n",
    "                                                  '持股市值占基金净值比(%)':'value_rate', \n",
    "                                                  '管理公司':'group'})\n",
    "            dataframe_fund = data_csv[['source','target','group','fluent_rate','value_rate']]\n",
    "            \n",
    "            #生成k值进一步完善dataframe_fund\n",
    "            a, b, c = hyper_params\n",
    "            dataframe_fund_copy = dataframe_fund.copy()\n",
    "            #生成k值\n",
    "            dataframe_fund_copy['k_value'] = dataframe_fund.apply(lambda row: round(a*row[3]+b*row[4]+c*row[3]*row[4], 2), axis=1)\n",
    "            dataframe_fund_copy.to_csv(data_csv_path, encoding='utf_8_sig', index=True)\n",
    "        else:\n",
    "            print('已存在',csv_name)\n",
    "            dataframe_fund_copy = pd.read_csv(data_csv_path, index_col = 0)\n",
    "        \n",
    "        #生成基金字典并储存基金网络\n",
    "        pkl_filepath = point_filepath + '/Pickle'\n",
    "        node_network_csv_path = point_filepath + '/' + csv_name + '_node_network' +'.csv'\n",
    "        fund_network_pkl_path = pkl_filepath +  '/' + csv_name + '_fund_network' +'.pkl'\n",
    "        if not os.path.exists(pkl_filepath):\n",
    "            try:\n",
    "                os.makedirs(pkl_filepath)\n",
    "            except:\n",
    "                continue\n",
    "        if os.path.exists(node_network_csv_path):\n",
    "            with open(fund_network_pkl_path,'rb') as f:   \n",
    "                fund_network = pkl.load(f)   \n",
    "        else:   \n",
    "            fund_network = generate_dict(dataframe_fund_copy, node_network_csv_path, fund_network_pkl_path)\n",
    "        fund_network_copy = deepcopy(fund_network)\n",
    "        threshold = hyper_threshold\n",
    "        \n",
    "        #生成制作网络的数据表\n",
    "        network_path = fund_filepath+'/'+csv_name+'.csv'\n",
    "        if not os.path.exists(network_path):\n",
    "            network_dataframe = generate_network_dataframe(fund_network,fund_network_copy,threshold)\n",
    "            network_dataframe.to_csv(network_path, encoding='utf_8_sig', index=True)\n",
    "        else:\n",
    "            network_dataframe = pd.read_csv(network_path, index_col = 0)\n",
    "\n",
    "        #生成共同重仓股列表\n",
    "        json_weighted_filepath = weighted_stock_filepath + '/Json'\n",
    "        weighted_order_csv_path = weighted_stock_filepath+'/'+csv_name+'.csv'\n",
    "        weighted_order_json_path = json_weighted_filepath+'/'+csv_name+'.json'   \n",
    "        fund_network_copy = deepcopy(fund_network)\n",
    "        if not os.path.exists(json_weighted_filepath):\n",
    "            try:\n",
    "                os.makedirs(json_weighted_filepath)\n",
    "            except:\n",
    "                continue\n",
    "        if not os.path.exists(weighted_order_csv_path):\n",
    "            weighted_stock_dataframe = generate_weighted_stock(fund_network,fund_network_copy,threshold)\n",
    "            with open(weighted_order_csv_path, 'w', encoding = 'utf_8_sig',newline='') as f:     \n",
    "                csv_write = csv.writer(f)\n",
    "                for new_line in weighted_stock_dataframe:\n",
    "                    csv_write.writerow(new_line) \n",
    "            with open(weighted_order_json_path,'w', encoding = 'utf_8_sig') as f:   \n",
    "                f.write(json.dumps(weighted_stock_dataframe))\n",
    "        else:\n",
    "            with open(weighted_order_json_path,'r', encoding = 'utf_8_sig') as f:   \n",
    "                weighted_stock_dataframe = f.read()\n",
    "                               ####用此方法可以将字典里的元素逐行读入；否则生成一个横向很长的列表。（转置作用）\n",
    "\n",
    "        #生成单独重仓股列表\n",
    "        json_important_filepath = important_stock_filepath + '/Json'\n",
    "        important_stock_csv_path = important_stock_filepath+'/'+csv_name+'.csv'\n",
    "        important_order_json_path = json_important_filepath+'/'+csv_name+'.json' \n",
    "        if not os.path.exists(json_important_filepath):\n",
    "            try:\n",
    "                os.makedirs(json_weighted_filepath)\n",
    "            except:\n",
    "                continue\n",
    "        if not os.path.exists(important_stock_csv_path):\n",
    "            important_stock_dataframe = generate_important_stock(fund_network,threshold)\n",
    "            with open(important_stock_csv_path, 'w', encoding = 'utf_8_sig',newline='') as f:     \n",
    "                csv_write = csv.writer(f)\n",
    "                for new_line in important_stock_dataframe:\n",
    "                    csv_write.writerow(new_line)   \n",
    "            with open(important_order_json_path,'w', encoding = 'utf_8_sig') as f:   \n",
    "                f.write(json.dumps(important_stock_dataframe))  \n",
    "        else:\n",
    "            with open(important_order_json_path,'r', encoding = 'utf_8_sig') as f:   \n",
    "                important_stock_dataframe = f.read()\n",
    "                \n",
    "        #将共同重仓股信息汇总在一张表\n",
    "        total_weighted_stock[csv_name] = ','.join([tuple_stock[0] for tuple_stock in weighted_stock_dataframe]) \n",
    "        \n",
    "        #将单独重仓股信息汇总在一张表\n",
    "        total_important_stock[csv_name] = ','.join([tuple_stock[0] for tuple_stock in important_stock_dataframe])  \n",
    "    \n",
    "    with open(total_filepath+'/'+'total_weighted_stock.csv', 'a', encoding = 'utf_8_sig') as total_weighted_stock_wirte:\n",
    "        for k,v in total_weighted_stock.items():\n",
    "            total_weighted_stock_wirte.writelines(','.join([k,v,'\\n']))\n",
    "    with open(total_filepath+'/'+'total_important_stock.csv', 'a', encoding = 'utf_8_sig') as total_important_stock_wirte:\n",
    "        for k,v in total_important_stock.items():\n",
    "            total_important_stock_wirte.writelines(','.join([k,v,'\\n']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-boutique",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 运行主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "above-secretary",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-74-604c1f3dd214>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-74-604c1f3dd214>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    #是返回数据已存在\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def main_fn(xlsx_filepath,  hyper_params, hyper_threshold, invest_category, year_category, mode = 'single'):\n",
    "    #生成标签文件夹来识别每一次\n",
    "    #不是重新生成数据\n",
    "    #是，判别年份\n",
    "    #不是， 不删除数据增加数据\n",
    "    #是返回数据已存在"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-routine",
   "metadata": {},
   "source": [
    "## 运行调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fb637ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数设置\n",
    "xlsx_filepath = './Dataset/StockData'\n",
    "\n",
    "hyper_params = [0.2, 0.8, 0.5]\n",
    "\n",
    "hyper_threshold = 8 \n",
    "\n",
    "worker = 4\n",
    "\n",
    "#year_category = ['','']\n",
    "\n",
    "invest_category =['普通股票型基金','指数增强型基金','灵活配置型基金','偏股混合型基金','平衡混合型基金','偏债混合型基金']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3430fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已建立 ./Dataset/StockData\n",
      "已建立 ./Dataset/StockData_csv\n",
      "已建立 ./Dataset/StockData_fund\n",
      "已建立 ./Dataset/StockData_point\n",
      "已建立 ./Dataset/StockData_weighted_stock\n",
      "已建立 ./Dataset/StockData_important_stock\n",
      "已建立 ./Dataset/StockData_total_stock\n",
      "正在生成普通股票型基金+指数增强型基金+灵活配置型基金+偏股混合型基金+平衡混合型基金+偏债混合型基金投资的网络\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2601b1f65a94e34bc6f0839256131f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xlsx_csv_fund(xlsx_filepath,  hyper_params, hyper_threshold, invest_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "respective-album",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已建立 ./Dataset/StockData\n",
      "已建立 ./Dataset/StockData_csv\n",
      "已建立 ./Dataset/StockData_fund\n",
      "已建立 ./Dataset/StockData_point\n",
      "已建立 ./Dataset/StockData_weighted_stock\n",
      "已建立 ./Dataset/StockData_important_stock\n",
      "已建立 ./Dataset/StockData_total_stock\n",
      "正在生成普通股票型基金+指数增强型基金+灵活配置型基金+偏股混合型基金+平衡混合型基金+偏债混合型基金投资的网络\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e154c33e0ad44695b62ca7528f719ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef2e545adca480796e3db04e3b0fda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f97bd1bc79f4390aad28b4d8bff0183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08411c616844dad9fc7831603f5ed58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xlsx_csv_fund_parallel(xlsx_filepath, hyper_params, hyper_threshold, worker, invest_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-frederick",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

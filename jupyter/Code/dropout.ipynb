{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader  \n",
    "from torch.nn import init\n",
    "import torch.optim as optim \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import time \n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, drop_prob):\n",
    "    X = X.float()\n",
    "    assert 0<=drop_prob<=1\n",
    "    keep_prob = 1- drop_prob\n",
    "    if keep_prob == 0:\n",
    "        return torch.zeros_like(X)\n",
    "    mask = (torch.rand(X.shape,dtype = torch.float)<keep_prob).float()\n",
    "    return mask*X/keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.arange(16).view(2,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "w1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens1)), dtype = torch.float,requires_grad = True)\n",
    "b1 = torch.zeros(num_hiddens1, requires_grad = True)\n",
    "w2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens1, num_hiddens2)), dtype = torch.float,requires_grad = True)\n",
    "b2 = torch.zeros(num_hiddens2, requires_grad = True)\n",
    "w3 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens2, num_outputs)), dtype = torch.float,requires_grad = True)\n",
    "b3 = torch.zeros(num_outputs, requires_grad = True)\n",
    "\n",
    "params = [w1, b1, w2, b2, w3, b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据切割\n",
    "def load_data_fashion_mnist(batch_size):\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root = './Dataset/FashionMNIST', train = True, \n",
    "                                                    download = True, transform = transforms.ToTensor())\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root = './Dataset/FashionMNIST', train = False, \n",
    "                                                   download = True, transform = transforms.ToTensor())\n",
    "    num_workers = 4\n",
    "    train_iter = DataLoader(mnist_train, batch_size = batch_size, shuffle = True, num_workers = num_workers)\n",
    "    test_iter = DataLoader(mnist_test, batch_size = batch_size, shuffle = True, num_workers = num_workers)\n",
    "    return train_iter, test_iter\n",
    "\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(X, params, drop_prob1, drop_prob2, is_training = True ):\n",
    "    X = X.view(-1, num_inputs)\n",
    "    w1, b1, w2, b2, w3, b3 = params\n",
    "    h1 = (torch.matmul(X, w1) + b1).relu()\n",
    "    if is_training:\n",
    "        h1 = dropout(h1, drop_prob1)\n",
    "    h2 = (torch.matmul(h1, w2) + b2).relu()\n",
    "    if is_training:\n",
    "        h2 = dropout(h2, drop_prob2)\n",
    "    return torch.matmul(h2, w3) + b3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    y_exp = y.exp()\n",
    "    partition = y_exp.sum(dim = 1, keepdim = True)\n",
    "    return (y_exp/partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr):\n",
    "    for param in params:\n",
    "        param.data -= lr*param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentrpyloss(y_hat, y):\n",
    "    return -(torch.log(y_hat.gather(1, y.view(-1,1))).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, network, params = None, drop_prob1 = 0.2, drop_prob2 = 0.5):\n",
    "    acc_num, n = 0, 0\n",
    "    for X, y in data_iter:\n",
    "        if isinstance(network, nn.Module):\n",
    "            network.eval()\n",
    "            acc_num += (network(X).argmax(dim = 1, keepdim = True) == y.view(-1,1)).float().sum().item()\n",
    "            network.train()\n",
    "        else:\n",
    "            if('is_training' in network.__code__.co_varnames):\n",
    "                acc_num += (network(X, params, drop_prob1, drop_prob2, is_training = False).argmax(dim = 1, keepdim = True) == y.view(-1,1)).float().sum().item()\n",
    "            else:\n",
    "                acc_num += (network(X).argmax(dim = 1, keepdim = True) == y.view(-1,1)).float().sum().item()\n",
    "        n += len(y)\n",
    "    return acc_num/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_dropout(n_epoch, drop_prob1, drop_prob2, \n",
    "                       train_iter, test_iter, \n",
    "                       model_fn, loss_fn, \n",
    "                       params = None, lr = None, optimizer = None):\n",
    "    for epoch in range(n_epoch):\n",
    "        train_loss, train_acc_num, n, i = 0.0, 0.0, 0, 0\n",
    "        for train_X, train_y in train_iter:\n",
    "            y_hat = softmax(model_fn(train_X, params, drop_prob1, drop_prob2))\n",
    "            loss = loss_fn(y_hat, train_y)\n",
    "            \n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if optimizer is not None:\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                sgd(params, lr)\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_acc_num += (y_hat.argmax(dim = 1) == train_y).float().sum().item()\n",
    "            n += len(train_y)\n",
    "            i+=1\n",
    "        if epoch%5 == 0:\n",
    "            test_loss, j = 0.0, 0\n",
    "            acc_test = evaluate_accuracy(test_iter, params, drop_prob1, drop_prob2, model_fn)\n",
    "            acc_train = train_acc_num/n\n",
    "            for test_X, test_y in test_iter:\n",
    "                j += 1\n",
    "                test_loss += loss_fn(softmax(model_fn(test_X, params, drop_prob1, drop_prob2, is_training = False)), test_y).item()\n",
    "            print('epoch: %d, acc_train: %.2f, acc_test: %.2f, train_loss: %.2f, test_loss, %.2f'% (epoch, acc_train, acc_test, train_loss/i, test_loss/j))\n",
    "    #return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc_train: 0.88, acc_test: 0.87, train_loss: 0.33, test_loss, 0.36\n",
      "epoch: 5, acc_train: 0.89, acc_test: 0.85, train_loss: 0.31, test_loss, 0.43\n",
      "epoch: 10, acc_train: 0.89, acc_test: 0.87, train_loss: 0.29, test_loss, 0.37\n",
      "epoch: 15, acc_train: 0.90, acc_test: 0.88, train_loss: 0.28, test_loss, 0.34\n"
     ]
    }
   ],
   "source": [
    "train_loop_dropout(n_epoch = 20, drop_prob1 = 0.2, drop_prob2 = 0.5, \n",
    "                       train_iter = train_iter, test_iter = test_iter, \n",
    "                       model_fn = network, loss_fn = crossentrpyloss, \n",
    "                       params = params, lr = 0.1, optimizer = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, X):\n",
    "        return X.view(X.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1, drop_prob2 = 0.2, 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = nn.Sequential(FlattenLayer(),\n",
    "                        nn.Linear(num_inputs, num_hiddens1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(drop_prob1),\n",
    "                        nn.Linear(num_hiddens1, num_hiddens2),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(drop_prob2),\n",
    "                        nn.Linear(num_hiddens2, num_outputs))\n",
    "\n",
    "for param in network.parameters():\n",
    "    nn.init.normal_(param, mean = 0, std = 0.01)\n",
    "\n",
    "optimizer = optim.SGD(network.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_dropout_nn(n_epoch, train_iter, test_iter, \n",
    "                       model_fn, loss_fn, \n",
    "                       params = None, lr = None, optimizer = None):\n",
    "    for epoch in range(n_epoch):\n",
    "        train_loss, train_acc_num, n, i = 0.0, 0.0, 0, 0\n",
    "        for train_X, train_y in train_iter:\n",
    "            y_hat = model_fn(train_X)\n",
    "            loss = loss_fn(y_hat, train_y)\n",
    "            \n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if optimizer is not None:\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                sgd(params, lr)\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_acc_num += (y_hat.argmax(dim = 1) == train_y).float().sum().item()\n",
    "            n += len(train_y)\n",
    "            i+=1\n",
    "        if epoch%5 == 0:\n",
    "            test_loss = 0.0\n",
    "            acc_test = evaluate_accuracy(test_iter, model_fn)\n",
    "            acc_train = train_acc_num/n\n",
    "            print('epoch: %d, acc_train: %.2f, acc_test: %.2f, test_loss, %.2f'% (epoch, acc_train, acc_test, train_loss/i))\n",
    "    #return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc_train: 0.79, acc_test: 0.76, test_loss, 0.60\n",
      "epoch: 5, acc_train: 0.85, acc_test: 0.85, test_loss, 0.44\n",
      "epoch: 10, acc_train: 0.87, acc_test: 0.84, test_loss, 0.38\n",
      "epoch: 15, acc_train: 0.88, acc_test: 0.85, test_loss, 0.34\n"
     ]
    }
   ],
   "source": [
    "train_loop_dropout_nn(n_epoch = 20, train_iter = train_iter, test_iter = test_iter, \n",
    "                       model_fn = network, loss_fn = nn.CrossEntropyLoss(), \n",
    "                       params = None, lr = None, optimizer = optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

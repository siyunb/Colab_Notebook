{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "optional-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data \n",
    "from torch.nn import init\n",
    "import torch.optim as optim \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import time \n",
    "import sys\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "configured-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a fliter from cross correlation\n",
    "def corr2d(X, K):\n",
    "    l, w = K.shape[0], K.shape[1]\n",
    "    Y = torch.zeros(X.shape[0]-l+1, X.shape[1]-w+1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j] = (X[i:i+l, j:j+w]*K).sum()\n",
    "    return Y   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "confident-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a fliter from convolutional view\n",
    "def corr2d_c(X, K):\n",
    "    l, w = K.shape[0], K.shape[1]\n",
    "    Y = torch.zeros(X.shape[0]-l+1, X.shape[1]-w+1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            summation = 0\n",
    "            for m in range(K.shape[0]):\n",
    "                for n in range(K.shape[1]):\n",
    "                    summation += X[i+m,j+n]*K[m,n]\n",
    "            Y[i,j] = summation\n",
    "    return Y    \n",
    "# I feel very  with this function\n",
    "# I fe function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "labeled-electricity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19., 25.],\n",
      "        [37., 43.]])\n",
      "tensor([[19., 25.],\n",
      "        [37., 43.]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[0,1,2],[3,4,5],[6,7,8]])\n",
    "K = torch.tensor([[0,1],[2,3]])\n",
    "print(corr2d(X, K))\n",
    "print(corr2d_c(X,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "seasonal-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size)) \n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "    def forward(self, X):\n",
    "        return self.corr2d(X, self.weight)+self.bias      \n",
    "    @staticmethod\n",
    "    def corr2d(X, K):\n",
    "        l, w = K.shape[0], K.shape[1]\n",
    "        Y = torch.zeros(X.shape[0]-l+1, X.shape[1]-w+1)\n",
    "        for i in range(Y.shape[0]):\n",
    "            for j in range(Y.shape[1]):\n",
    "                Y[i,j] = (X[i:i+l, j:j+w]*K).sum()\n",
    "        return Y  \n",
    "    \n",
    "#@classmethod隐含class的self被当作参数\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fixed-operator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(8,8, dtype = torch.float)\n",
    "X[:, 2:6] = 0.0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "static-prize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "K = torch.tensor([[1,-1]])\n",
    "Y = Conv2D.corr2d(X, K)\n",
    "print(Y)\n",
    "#it can represent the feature of space very well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "southeast-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = Conv2D((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "retained-satellite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4, loss:7.197460174560547\n",
      "epoch:9, loss:0.9880686402320862\n",
      "epoch:14, loss:0.16810810565948486\n",
      "epoch:19, loss:0.02930174581706524\n",
      "epoch:24, loss:0.005121978931128979\n",
      "epoch:29, loss:0.0008957354002632201\n",
      "tensor([[ 0.9937, -0.9937]]) tensor([2.2205e-05])\n"
     ]
    }
   ],
   "source": [
    "# through the supervision learning to learn the parameter of the kernel\n",
    "# the standard kernel K\n",
    "K = torch.tensor([[1,-1]])\n",
    "Y = Conv2D.corr2d(X, K) # recall the staticmethod\n",
    "\n",
    "num_epoch = 30 \n",
    "lr = 0.01      \n",
    "conv2d = Conv2D((1,2))     \n",
    "\n",
    "#upgrade the kernel in conv2d\n",
    "for epoch in range(num_epoch):\n",
    "    Y_hat = conv2d(X)\n",
    "    loss = ((Y - Y_hat)**2).sum()\n",
    "    \n",
    "    if conv2d.weight.grad is not None:\n",
    "        for _, param in conv2d.named_parameters():\n",
    "            param.grad.zero_()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    for param in conv2d.parameters():\n",
    "        param.data -= lr*param.grad\n",
    "    \n",
    "    if (epoch+1)%5 == 0:\n",
    "        print('epoch:{0}, loss:{1}'.format(epoch, loss))\n",
    "print(conv2d.weight.data, conv2d.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "spiritual-greece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.4457, -0.0815]], requires_grad=True)),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([1.2588], requires_grad=True))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(conv2d.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "provincial-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding and Stride\n",
    "X = torch.rand(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "sudden-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 3, padding = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "veterinary-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_conv2d(conv2d_c, x_c):\n",
    "    x_c = x_c.view((1,1)+x_c.shape)\n",
    "    y = conv2d_c(x_c)\n",
    "    return y.view(y.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "graduate-silence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "muslim-strip",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size = (5,3), padding=(2,1))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "approximate-watershed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2330, -0.6291],\n",
       "        [-0.3928, -0.5024]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size = (3,5), padding=(0,1), stride=(3, 4))\n",
    "comp_conv2d(conv2d,X)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "conceptual-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a fliter from cross correlation\n",
    "def corr2d(X, K):\n",
    "    l, w = K.shape[0], K.shape[1]\n",
    "    Y = torch.zeros(X.shape[0]-l+1, X.shape[1]-w+1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j] = (X[i:i+l, j:j+w]*K).sum()\n",
    "    return Y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "settled-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X,K):\n",
    "    res = corr2d(X[0,:,:], K[0,:,:])\n",
    "    for i in range(1, X.shape[0]):\n",
    "        res += corr2d(X[i,:,:], K[i,:,:])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "baking-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[[0,1,2],[3,4,5],[6,7,8]],[[1,2,3],[4,5,6],[7,8,9]]])\n",
    "K = torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "promotional-switch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in(X,K).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "veterinary-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "independent-combine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "K = torch.stack([K,K+1,K+2])\n",
    "for k in K:\n",
    "    print(k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "restricted-conflict",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X, K).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "union-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1_1(X, K):\n",
    "    c,h,w = X.shape\n",
    "    X = X.view(c,-1)\n",
    "    K = K.view(K.shape[0],-1)\n",
    "    Y = torch.mm(K,X)\n",
    "    return Y.view(K.shape[0], h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "recovered-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(3,3,3)\n",
    "K = torch.rand(4,3,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "emotional-concern",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 3])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out_1_1(X,K).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "after-advocacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         [ True, False,  True]],\n",
       "\n",
       "        [[ True,  True,  True],\n",
       "         [ True,  True, False],\n",
       "         [False,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True],\n",
       "         [ True, False, False],\n",
       "         [ True,  True, False]],\n",
       "\n",
       "        [[ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X,K) == corr2d_multi_in_out_1_1(X,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "scenic-apartment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype = torch.float).view((1,1,4,4))\n",
    "pool2d = nn.MaxPool2d(3) #default set the same as the stride\n",
    "pool2d(X)\n",
    "pool2d=nn.MaxPool2d(2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "becoming-third",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  3.],\n",
       "          [ 9., 11.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(kernel_size=(2,4),padding=(1,2),stride=(2,3))\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "small-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cat((X, X+1),dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "least-sweden",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]],\n",
       "\n",
       "         [[ 6.,  8.],\n",
       "          [14., 16.]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(kernel_size=3,padding=1,stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "treated-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return x.view(x.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caring-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "                                  nn.Sigmoid(),\n",
    "                                  nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "                                  nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5),\n",
    "                                  nn.Sigmoid(),\n",
    "                                  nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "                                 )\n",
    "        self.flatten = nn.Sequential(Flatten())\n",
    "        self.fc = nn.Sequential(nn.Linear(16*4*4, 120),\n",
    "                               nn.Sigmoid(),\n",
    "                               nn.Linear(120,84),\n",
    "                               nn.Sigmoid(),\n",
    "                               nn.Linear(84,10))\n",
    "    def forward(self,X):\n",
    "        conv_feature = self.conv(X)\n",
    "        flat_feature = self.flatten(conv_feature)\n",
    "        output = self.fc(flat_feature)\n",
    "        return output \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "polish-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size):\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root = './Dataset/FashionMNIST', train = True, \n",
    "                                                    download = True, transform = transforms.ToTensor())\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root = './Dataset/FashionMNIST', train = False, \n",
    "                                                   download = True, transform = transforms.ToTensor())\n",
    "    if sys.platform.startswith('win'):\n",
    "        num_workers = 4\n",
    "    else:\n",
    "        num_workers = 0\n",
    "    train_iter = Data.DataLoader(mnist_train, batch_size = batch_size, shuffle = True, num_workers = num_workers)\n",
    "    test_iter = Data.DataLoader(mnist_test, batch_size = batch_size, shuffle = True, num_workers = num_workers)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "organizational-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "loving-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, network, device = None):\n",
    "    acc_num, n = 0.0, 0\n",
    "    if device is None:        \n",
    "        device = list(network.parameters())[0].device\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter: \n",
    "            network.eval()\n",
    "            num = ((network(X.to(device))).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "            acc_num += num \n",
    "            network.train()\n",
    "            n += y.shape[0]\n",
    "    return acc_num/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "sticky-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(network, train_iter, test_iter, optimizer, num_epochs): \n",
    "    device = 'cuda' if torch.cuda.is_available else 'cpu' \n",
    "    network = network.to(device)\n",
    "    print('training on ', device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_sum, train_acc_num, test_acc, batch_count, n, start_time = 0.0, 0.0, 0.0, 0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = network(X)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_sum += loss.cpu().item() \n",
    "            train_acc_num += (y_hat.argmax(dim=1)== y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        train_acc = train_acc_num/n\n",
    "        train_loss = train_loss_sum/batch_count\n",
    "        test_acc += evaluate_accuracy(test_iter, network)\n",
    "        for X_test, y_test in test_iter:\n",
    "            test_loss_sum, batch_count_test = 0.0, 0\n",
    "            X_test = X_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "            with torch.no_grad():\n",
    "                y_test_hat = network(X_test)\n",
    "                test_loss_sum += loss_fn(y_test_hat, y_test).cpu().item() \n",
    "            batch_count_test += 1\n",
    "        test_loss = test_loss_sum/batch_count_test\n",
    "        print('epoch: %d, time: %.3f, train_loss: %.3f, test_loss: %.3f, train_acc: %.2f, test_acc: %.2f'\n",
    "              %(epoch+1, (time.time()-start_time), train_loss, test_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "british-suite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): Sigmoid()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Sequential(\n",
      "    (0): Flatten()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=120, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "network = LeNet()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "framed-baptist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda:0\n",
      "epoch: 1, time: 4.166, train_loss: 1.834, test_loss: 0.852, train_acc: 0.32, test_acc: 0.60\n",
      "epoch: 2, time: 4.107, train_loss: 0.941, test_loss: 0.550, train_acc: 0.64, test_acc: 0.68\n",
      "epoch: 3, time: 4.080, train_loss: 0.774, test_loss: 0.842, train_acc: 0.72, test_acc: 0.72\n",
      "epoch: 4, time: 4.058, train_loss: 0.685, test_loss: 0.863, train_acc: 0.74, test_acc: 0.74\n",
      "epoch: 5, time: 3.988, train_loss: 0.630, test_loss: 0.309, train_acc: 0.76, test_acc: 0.75\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = optim.Adam(network.parameters(), lr=lr)\n",
    "training_loop(network, train_iter, test_iter, optimizer, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-finance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
